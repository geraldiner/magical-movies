comments database
- comments from reddit

{
  author: who made the comment,
  body: the text of the comment,
  id: id of the specific comment,
  permalink: link to the comment,
  score: supposedly upvotes but idk??? (will be used to rank the movies)
}

movies database
- based on the comments from reddit
- linked to IMDB or other movie API - get info about it to store
https://developers.themoviedb.org/3/getting-started/introduction

won't get rate-limited

https://api.themoviedb.org/3/search/movie?api_key=f3c99db396389b38dd7348a417e14b69&language=en-US&query=<query>

{
    "page": 1,
    "results": [
        {
            "adult": false,
            "backdrop_path": "/sqk5jLiyTDd9AHfrDdHBf1ecmte.jpg",
            "genre_ids": [
                35,
                10751
            ],
            "id": 6283,
            "original_language": "en",
            "original_title": "MouseHunt",
            "overview": "Down-on-their luck brothers, Lars and Ernie Smuntz, aren't happy with the crumbling old mansion they inherit... until they discover the estate is worth millions. Before they can cash in, they have to rid the house of its single, stubborn occupant—a tiny and tenacious mouse.",
            "popularity": 14.806,
            "poster_path": "/aqBPrWOzXEO3rWEk3DYTHBjXNZb.jpg",
            "release_date": "1997-12-19",
            "title": "MouseHunt",
            "video": false,
            "vote_average": 6.4,
            "vote_count": 848
        }
    ],
    "total_pages": 1,
    "total_results": 1
}

Reddit thread
https://www.reddit.com/r/datascience/comments/dhchq2/extracting_movie_titles_from_text_source/
The concept you're looking for is called named entity recognition. Here's what NLTK has to say: https://www.nltk.org/book/ch07.html

Do this to create a labelled dataset. Split this dataset and build a custom NER model in Spacy. When you are comfortable with these steps in spacy, go ahead and build a CRF+LSTM model.

https://sematext.com/blog/entity-extraction-with-spacy/

NER = named entity recognition

1. use pre-built model to extract entities, then build own model

pre-built model
1. download: python -m spacy download en_core_web_sm (english)
2. load in script: nlp = spacy.load('en_core_web_sm')
3. extract: doc = nlp(text)
	for entity in doc.ents:
		# do stuff

train a new model
1. create a pipeline that defines how to process data
spaCy piplines
- maybe pre-process text (eg. via stemming) or tagging (part of speech)

nlp = spacy.blank('en')  # new, empty model. Let’s say it’s for the English language
nlp.vocab.vectors.name = 'example_model_training'   # give a name to our list of vectors
# add NER pipeline
ner = nlp.create_pipe('ner')  # our pipeline would just do NER
nlp.add_pipe(ner, last=True)  # we add the pipeline to the model


Take the text of the comment and textual analysis - print results with the id attached?